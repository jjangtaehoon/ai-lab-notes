# AI 시스템 실험 로그 (AI Systems Experiment Log)

이 저장소는 개인적으로 진행해온 **AI / LLM 기반 시스템 실험 기록**을 정리한 공간입니다.  
모델 성능이나 데모 구현이 아니라, **실제 시스템 안에서 AI를 사용했을 때 어디까지 가능한지, 어디서 반드시 사람이 개입해야 하는지**를 기록하는 것을 목표로 합니다.

완성된 제품이나 라이브러리를 만드는 목적은 아니며,  
AI를 **현실적인 시스템 구성 요소 중 하나**로 다뤄보며 얻은 판단과 한계를 남기는 **기술 실험 로그**에 가깝습니다.

---

## 이 저장소를 만든 이유

최근의 AI 관련 자료 대부분은 다음에 집중합니다.

- 더 좋은 모델
- 더 높은 벤치마크
- 더 그럴듯한 데모

하지만 실제 서비스나 시스템 환경에서는  
**모델 성능보다 훨씬 중요한 문제들이 뒤에서 발생합니다.**

이 저장소는 다음 질문에서 출발합니다.

- 이 AI 출력은 언제부터 위험해지는가?
- 자동화해도 되는 지점과, 반드시 규칙으로 고정해야 할 지점은 어디인가?
- LLM을 어디까지 신뢰할 수 있고, 어디서 통제해야 하는가?
- “된다”는 말이 실제 운영 환경에서도 의미가 있는가?

이 저장소는 그런 질문에 대한 **실험과 실패 기록**을 남기기 위해 만들어졌습니다.

---

## 기본 관점

- AI 모델을 맹신하지 않습니다
- LLM은 결정을 대신하는 존재가 아니라 **사고를 보조하는 도구**로 사용합니다
- 결과보다 **과정, 판단 근거, 실패 지점**을 중요하게 봅니다
- “성공 사례”보다 **깨지는 지점**을 더 가치 있게 기록합니다
- AI는 독립적인 지능이 아니라 **통제되어야 할 시스템 컴포넌트**라고 봅니다

---

## 다루는 실험 범위

이 저장소에서는 다음과 같은 주제를 다룹니다.

- LLM을 실제 시스템 파이프라인에 연결하며 발생하는 문제
- 검색 시스템에서 벡터 검색과 키워드 검색의 한계 비교
- 임베딩 전략, 재인덱싱 비용, 성능-운영 트레이드오프
- 컨텍스트 오염, 프롬프트 누수, 범위 통제 문제
- 자동화가 실패하는 지점과 규칙 기반 처리가 필요한 영역
- AI 출력 해석 시 사람이 직접 판단해야 했던 사례

※ 대부분의 실험은 공개 모델 및 공개 데이터를 기반으로 합니다.

---

## 실험 기록 방식

각 실험은 보통 다음 내용을 포함합니다.

- 실험을 하게 된 실제 문제 상황
- 모델이나 도구를 선택한 이유
- 시스템에 어떻게 연결했는지
- 관찰된 결과와 예상과 달랐던 지점
- 모델이 처리하지 못해 사람이 개입해야 했던 판단 포인트

코드의 양이나 완성도보다는,  
**왜 이런 선택을 했고, 어디서 문제가 발생했는지**가 드러나는 기록을 남기는 데 집중합니다.

---

## 이 저장소가 지향하지 않는 것

- ❌ 유행을 따라가는 AI 데모
- ❌ 모델 성능 비교표
- ❌ “이 모델이 최고다”식의 결론

---

## 이 저장소가 지향하는 것

- 실제 환경에서 AI를 사용할 때의 현실적인 한계 인식
- 자동화와 책임의 경계를 명확히 나누는 감각
- AI를 시스템 안에 넣기 전에 반드시 검토해야 할 판단 기준
- **AI를 통제하고 관리하는 엔지니어의 시선**

---

## 참고 사항

- 본 저장소는 개인적인 기술 실험 기록입니다.
- 재직 중인 회사나 기관의 프로젝트, 소스 코드, 데이터와는 무관합니다.
- 실험 내용은 진행 과정에 따라 수정·추가될 수 있습니다.
