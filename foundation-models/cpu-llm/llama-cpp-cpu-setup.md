# CPU 기반 LLM 실험 환경 구축 (llama.cpp)

## 실험 목적
GPU가 없는 공용 서버 환경에서
LLM을 내부 시스템 컴포넌트로 사용하기 위한
현실적인 실행 방식을 검증한다.

모델 성능보다
- 데이터 통제
- 운영 안정성
- 환경 독립성
을 우선 기준으로 삼는다.

## 환경 제약
- GPU 없음
- CentOS 7 (공용 서버)
- 시스템 gcc 업그레이드 불가
- CPU/메모리는 충분

## 선택 이유
위 조건에서
llama.cpp + GGUF 양자화 모델이
유일하게 현실적인 선택지였다.

## 모델 기준
- 포맷: GGUF
- 크기: 7B
- 양자화: Q4
- Instruct 튜닝 모델

→ 품질/속도/메모리 균형상 7B Q4를 기준 모델로 고정

## 실행 구조
[모델]
Qwen / LLaMA / Mistral / Gemma (GGUF)

↓
[실행 엔진]
llama.cpp

↓
[사용 방식]
- CLI
- HTTP 서버
- Python subprocess

→ 모델 교체 시 실행 방식은 그대로 유지

## 빌드 전략 (중요 판단 포인트)
- 전역 gcc 변경 불가
- conda 환경 내부에서만 컴파일러 사용
- 시스템 환경에 영향 없는 방식 채택

## 운영 관점에서의 장점
- GPU 불필요
- 실행/종료 명확
- 자원 점유 시간 제어 가능
- 데이터 외부 유출 없음

## 한계
- 추론 속도는 GPU 대비 느림
- 장문 컨텍스트 비용 큼
- 고난도 추론 성능은 제한적

## 사람이 개입해야 하는 지점
- 모델 크기/양자화 선택
- 실행 시점 제어
- 시스템 통합 시 호출 방식 결정

👉 [llama.cpp CPU 환경 구축 노트](./_notes/llama-cpp-cpu-setup.md)
