# LLM 프롬프트 기반 자연어 → 논리식 변환 실험 기록

본 문서는  
PHR(개인 건강 데이터) 조건을 자연어로 입력받아  
논리 수식으로 변환하는 UI/LLM 연동 실험 과정과  
그 과정에서 확인한 **프롬프트 설계·컨텍스트 관리·시스템 한계**를 기록한 실험 로그이다.

모델 성능 비교가 목적이 아니라,  
**실제 서비스 UI에서 LLM을 사용할 때 어디까지 자동화가 가능하고  
어디서부터 사람이 통제해야 하는지**를 확인하는 것이 목적이다.

---

## 실험 목적

- 사용자가 자연어로 입력한 조건을 논리식으로 변환
- 괄호 구조, 필드명, 비교 연산자를 최대한 유지
- UI 기반 대화 흐름에서 LLM 응답의 일관성 관찰
- 프롬프트·컨텍스트 길이에 따른 응답 품질 차이 확인

---

## 사용 환경 개요

- 프론트엔드: React + Ant Design
- 입력 방식: 대화형 UI (Modal)
- LLM 호출 방식: HTTP API (llama-server)
- 실행 모델: 로컬 CPU 기반 LLM (llama.cpp)
- 서버 통신: fetch 기반 단일 completion 요청

---

## 시스템 프롬프트 설계

LLM의 역할을 **“자연어 조건을 논리식으로 변환하는 엔진”**으로 고정하고,  
다음과 같은 규칙을 시스템 프롬프트에 명시하였다.

- 필드명과 괄호 구조를 최대한 유지
- AND / OR를 각각 ∧ / ∨ 로 변환
- 이상·이하·초과·미만을 기호로 치환
- 출력은 **설명 없이 수식 한 줄만 반환**
- 예시 입력/출력을 포함하여 출력 형식 강제

이 단계에서 확인한 점은,  
**규칙을 명확히 적지 않으면 LLM은 쉽게 설명을 덧붙이거나 형식을 벗어난다**는 것이다.

---

## 컨텍스트 관리 전략

대화형 UI 특성상 모든 대화를 컨텍스트로 전달할 경우:

- 프롬프트 길이 증가
- 이전 출력에 과도하게 끌려가는 현상
- 응답 일관성 저하

이를 방지하기 위해 다음 전략을 사용하였다.

- 시스템 프롬프트는 항상 포함
- 직전 사용자 입력과 직전 출력만 컨텍스트에 포함
- 전체 대화 히스토리는 UI에만 유지

이 방식은 응답 안정성과 토큰 사용량 측면에서 가장 균형이 좋았다.

---

## 관찰된 결과

### 긍정적인 점

- 단순 조건은 높은 정확도로 논리식 변환 가능
- 괄호가 포함된 조건도 비교적 안정적으로 처리
- temperature를 낮게 설정할수록 일관성 향상

### 문제점

- 조건이 길어질수록 괄호 해석 오류 가능성 증가
- “이고 / 이거나” 혼합 조건에서 우선순위 혼동 발생
- 사용자가 애매한 자연어를 입력하면 모델이 임의 해석 시도

---

## UI 연동에서 드러난 한계

- LLM 응답이 틀려도 겉보기에는 그럴듯함
- 잘못된 수식을 그대로 적용하면 치명적인 정책 오류 가능
- 네트워크 오류, 서버 응답 지연 시 사용자 경험 저하

이를 보완하기 위해:

- 마지막 생성 수식을 별도로 강조 표시
- 오류 응답에 대해 재시도 UI 제공
- “대화 초기화” 기능 제공

---

## 사람이 반드시 개입해야 하는 지점

- 생성된 논리식의 최종 검증
- 복잡한 조건에 대한 구조적 분해 여부 판단
- LLM 출력값을 그대로 정책에 반영할지 결정
- 자동화 범위를 어디까지 허용할지에 대한 정책 결정

---

## 정리

이 실험을 통해 확인한 핵심은 다음과 같다.

- LLM은 **수식 변환을 대신 판단해주는 존재가 아니라**
  **사람의 의도를 구조화해주는 보조 도구**에 가깝다.
- 프롬프트 설계와 컨텍스트 제한이 품질을 좌우한다.
- UI와 결합된 LLM은 반드시 실패 시나리오를 전제로 설계해야 한다.

따라서 본 기능은  
“완전 자동화”가 아니라  
**사람의 검증을 전제로 한 반자동 도구**로 사용하는 것이 적절하다.

👉 자세한 실험 기록은 아래 노트를 참고한다.

- [LLM 프롬프트 관찰 노트](./_note/llm-prompt-observation.md)
